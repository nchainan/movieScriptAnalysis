{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Movie Scripts - An Analysis\n",
    "### Act 3\n",
    "_\"Maybe I didn't want the antidote.\"_\n",
    "\n",
    "In the final act, we try to generate some dialogue using some machine learning techniques. Let's see if we can get it to sound like Hollywood quality.\n",
    "\n",
    "### 1) Imports and data loading!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_df = pickle.load(open(\"movies_df.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_thrillers = \"\"\n",
    "for t in movies_df[movies_df.genre == \"Thriller\"].condensed_text:\n",
    "    for line in [l.lstrip() for l in t]:\n",
    "        all_thrillers += line\n",
    "data = all_thrillers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take all the thriller movies and see if we can write some of our own! We'll user a character based RNN/LSTM, where we take sequences of length 120 and try to predict the next character. Hopefully we get some good stuff out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 51456649 characters, 98 unique.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-5aa614657dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level  RNN model. Adapted from by Andrej Karpathy (@karpathy)\n",
    "\"\"\"\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# credit michaelrzhang Char-RNN\n",
    "maxlen = 120\n",
    "step = 13\n",
    "sentences = []\n",
    "targets = []\n",
    "for i in range(0,len(data)-maxlen-1, step):\n",
    "    sentences.append(data[i:i+maxlen])\n",
    "    targets.append(data[i+maxlen])\n",
    "print('number of sequences:', len(sentences))\n",
    "\n",
    "X = np.zeros((1000000, maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((1000000, len(chars)), dtype=np.bool)\n",
    "for i in range(1000000):\n",
    "    sentence = sentences[i]\n",
    "    target = targets[i]\n",
    "    y[i][char_to_ix[target]] = 1\n",
    "    for j in range(maxlen):\n",
    "        X[i][j][char_to_ix[sentence[j]]] = 1\n",
    "        \n",
    "        \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen,len(chars))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(chars), activation = \"softmax\"))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy')\n",
    "\n",
    "# train and print out test text\n",
    "start = np.random.randint(0, 999998)\n",
    "for i in range(50):\n",
    "    print (\"-\" * 10) + \"Epoch \" + str(i) + (\"-\" * 10)\n",
    "    model.fit(X, y, epochs=1, verbose=0)\n",
    "    total_string = sentences[start]\n",
    "    input_vec = X[start]\n",
    "    for i in range(10):\n",
    "        x = input_vec.reshape((1, input_vec.shape[0], input_vec.shape[1]))\n",
    "        pred = model.predict(x)\n",
    "        c = ix_to_char[pred.argmax()]\n",
    "        total_string += c\n",
    "        input_vec = np.vstack((input_vec[1:], [True if i == pred.argmax() else False for i in range(len(chars))]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I don't have the processing power required to make this come to life - I'm running this using the CPU on my Macbook, rather than a GPU, so its taking about 4 hours per epoch and after 5 epochs the results are kinda crappy. I've included the code above so I can run it another time.\n",
    "\n",
    "**Instead, I'm going to try a simpler, more comprehensible model - A 2-character Markov Model.**\n",
    "\n",
    "This means that I'll be predicting what a character will be given the two preceeding it. For example, given the characters _th_, there could be a 30% chance it's followed by an _a_ and 70% chance it's followed by an _e_, so we select the next character from this probability distribution.\n",
    "\n",
    "I'm sure there are packages to do this, but I'll write one from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start with breaking the string down into ((a,b), c) pairs\n",
    "from collections import defaultdict\n",
    "transition_dict = defaultdict(lambda : defaultdict(int)) # create a defaultdict of defaultdict\n",
    "for i in range(len(all_thrillers)-2): # for each triplet of characters\n",
    "    transition_dict[(data[i], data[i+1])][data[i+2]] += 1 # create a key for (a,b) and dict for following letters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we really need to generate the model. From here, we'll start with a random seed and then stochastically choose the subsequent characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT.                                                                                                                 CONT'D)\n",
      "Nothe pare sten ready.\n",
      "MER\r\n",
      "and the on and me pit the call be the her the as \n",
      "the ing whe liket ther ing there dow dow mand the and the pas hickes pay so got and to the ge a dow ithe a sing the there sidend loom on to his and of ther he les the of the bacre anto ing to ling seets and ing the a dow his and nothe got loses all the the side.\n",
      "I dooke ther there the of the dook and se wor the withe st wholeat the ou lif the se cone st hand he frout ing withe the you and you'readin st the the firs the ple the like younto the sure cand shat ound the staread whoully and to the ing a me the cars som ne a for the withe mallower all the door and she ing of the strand yout then to sone come re re she seento ther to met knothe the a pack the and ou to to his hout ther ones the of the wall mand st the puld the but ther of the the and to a be ing the of the and he st the be one a\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"IN\"\n",
    "total_string = seed_text\n",
    "for i in range(1000):\n",
    "    key = total_string[-2:] # get last two letters\n",
    "    val_dict = transition_dict[(key[0],key[1])] # retrieve dictionary for values\n",
    "    counts_sq = [x**2 for x in val_dict.values()]\n",
    "    prob_dist = [1.*val/np.sum(counts_sq) for val in counts_sq] # get prob dist\n",
    "    next_char = np.random.choice(val_dict.keys(), p=prob_dist) # choose randomly\n",
    "    total_string += next_char\n",
    "print total_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty nonsensical. Let's take up the n in n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "# start with breaking the string down into ((a,b), c) pairs\n",
    "from collections import defaultdict\n",
    "transition_dict = defaultdict(lambda : defaultdict(int)) # create a defaultdict of defaultdict\n",
    "for i in range(len(all_thrillers)-n-1): # for each triplet of characters\n",
    "    tup = tuple(data[i+j] for j in range(n)) # create arbitrary length tuple\n",
    "    transition_dict[tup][data[i+n+1]] += 1 # create a key for (a,b,...) and dict for following letters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be non-empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-361-9a85d69973a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcounts_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprob_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_sq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts_sq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get prob dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_dist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# choose randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtotal_string\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtotal_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be non-empty"
     ]
    }
   ],
   "source": [
    "seed_text = \"The\"\n",
    "total_string = seed_text\n",
    "for i in range(100):\n",
    "    key = total_string[-n:] # get last two letters\n",
    "    tup = tuple(key[j] for j in range(n))\n",
    "    val_dict = transition_dict[(tup)] # retrieve dictionary for values\n",
    "    counts_sq = [x**2 for x in val_dict.values()]\n",
    "    prob_dist = [1.*val/np.sum(counts_sq) for val in counts_sq] # get prob dist\n",
    "    next_char = np.random.choice(val_dict.keys(), p=prob_dist) # choose randomly\n",
    "    total_string += next_char\n",
    "print total_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that there are so many tri-grams without anything after that the sequence just fizzle outs. Let's try out word-level ngrams instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tokens = data.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with breaking the string down into ((a,b), c) pairs\n",
    "from collections import defaultdict\n",
    "transition_dict_words = defaultdict(list) # create a defaultdict of defaultdict\n",
    "for i in range(len(word_tokens)-2): # for each triplet of characters\n",
    "    transition_dict_words[(word_tokens[i], word_tokens[i+1])].append(word_tokens[i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaving\n",
      "a small cut.\n",
      "Del starts twitching, rocking back and forth.\n",
      "He slams the door shut.\n",
      "Then Yoko moves through the air, a moment with images\n",
      "from those \"VISIONS\" that have nothing further to\n",
      "discuss.\n",
      "Bruno goes to the floor, do it if you didn't insist on calling all the \n",
      "pleasure they are not harmless misfits.\n",
      "What you see me?\n",
      "SOFIA\n",
      "Maybe I didn't want the antidote.\n",
      "VERONA\n",
      "Oh, the antidote, huh?\n",
      "VERONA makes eye contact going on here.\r\n",
      "28 INTERIOR ADM- THE NEXT DAY            93\n",
      "The Doctor can't face it.  I know something. (his \n",
      "voice goes up too.\r\n",
      "Allison looks at him a picture)\r\n",
      "Kathy's. I\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_text = list(random.choice(transition_dict_words.keys()))\n",
    "total_string = seed_text\n",
    "for i in range(100):\n",
    "    key = total_string[-2:] # get last two letters\n",
    "    val_dict = transition_dict_words[(key[0],key[1])] # retrieve dictionary for values\n",
    "    next_char = np.random.choice(val_dict) # choose randomly\n",
    "    total_string.append(next_char)\n",
    "print ' '.join(total_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like this better than using NLTK's word tokenizer, because this way we can preserve structure like new lines and tabs, rather than just joining all word tokens with a space. For example, \"here.\\nCan\" is a single token.\n",
    "\n",
    "It's interesting how the thriller genre comes out pretty strong. Death, pressure, frightening themes are present. What's clear though is that we are likely to get stuck in a sequence of direct matches from scripts, if only a two-gram leads to a single term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "#### And there you have it!\n",
    "This concludes the third act in a movie script analysis series. This act was particularly resource-constrained, and as such the auto-generated script didn't have much of a polished feel to it. The next iteration would explore the deep learning route - tuning the perfect LSTM to generate realistic sounding scripts. We could also try different script cuts, like other genres, or even try to get generate dialogue from specific characters (provided there's enough material). Looking forward to trying out ideas other may have using this data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
